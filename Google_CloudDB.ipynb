{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import mysql.connector\n",
    "from mysql.connector.constants import ClientFlag\n",
    "import zipfile\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import glob\n",
    "from collections import namedtuple\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to process data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function\n",
    "\n",
    "Author = namedtuple('Author', 'guid name email source location imageUrl coverUrl')\n",
    "Visual=namedtuple('Visual', 'guid type contentType generatingRenditions url caption credit language')\n",
    "Article=namedtuple('Article', 'guid templateName availableInPreview url creationDate modificationDate title channel lead chapters')\n",
    "\n",
    "def process_analytic_file(thisPath,thisFile):\n",
    "    \"\"\"\n",
    "    this function specifically transforms the inputs of the analytic file of GLU7027\n",
    "    it transforms the dict structure into tuples and performs the following modifs\n",
    "    \n",
    "    1;split the name into 2 parts  product-->the newspaper's name|plateform\n",
    "    2.transform created_at date into a mysql readable format\n",
    "    3.transforms the name into an integer value ex: View60-->60\n",
    "    \"\"\"\n",
    "    path=thisPath+thisFile\n",
    "    with open(path) as jar:\n",
    "        data = json.load(jar)\n",
    "        output=[]\n",
    "        for d in data:\n",
    "            d['createdAt']=d['createdAt'][:-1]\n",
    "            temp=d['product'].split('/') # to create additional column for the type of platform\n",
    "                \n",
    "            d['product']=temp[0]\n",
    "            d['interface']=temp[1]\n",
    "            #just get the number part the \"view\" has no purpose\n",
    "            if len(d['name'])==4:\n",
    "                d['name']=0\n",
    "            elif len(d['name'])==5:\n",
    "                d['name']=int(d['name'][-1:])\n",
    "            else:\n",
    "                d['name']=int(d['name'][-2:])\n",
    "        jar.close() \n",
    "        return [tuple(d.values()) for d in data]\n",
    "    \n",
    "\n",
    "def process_article_file(thisPath,thisFile):\n",
    "    \"\"\"\n",
    "    this function transform the JSON of an article into 3 namedTuples, one for the article,the author and\n",
    "    the visuals in the article.\n",
    "    \n",
    "    this function processes one article at a time\n",
    "    \n",
    "    inputs:it uses the folder path and file name as inputs\n",
    "    outputs: a list of Authors namedTuples\n",
    "             a list of Visual named tuples\n",
    "             a list of Articles named tuples\n",
    "\n",
    "    \n",
    "    author and visual use the guid of the article as its foreign key\n",
    "    Because it is a nested json and not all json have all the information, exception handling was necessary\n",
    "    to avoid the pyramid of doom\n",
    "    \n",
    "    \"\"\"\n",
    "    path=thisPath+thisFile\n",
    "    with open(path) as jar:\n",
    "        data = json.load(jar)\n",
    "        articles=[]\n",
    "        authors=[]\n",
    "        visuals=[]\n",
    "        \n",
    "        #author part:\n",
    "        for author in data['authors']:\n",
    "           \n",
    "            authorKeys=list(author.keys())\n",
    "            guid=data['id'] #it will be it foreign key to link back article to author\n",
    "            \n",
    "            if 'name' in authorKeys:\n",
    "                name=author['name']\n",
    "            else:\n",
    "                name='Elvis Gratton'\n",
    "            if 'email' in authorKeys:\n",
    "                email=author['email']\n",
    "            else:\n",
    "                email=\"bobGratton@garabeGratton.com\"\n",
    "            if 'source' in authorKeys:\n",
    "                source=author['source']\n",
    "            else:\n",
    "                source=''\n",
    "            if 'location' in authorKeys:\n",
    "                location=author['location']\n",
    "            else:\n",
    "                location='Lacheneuil'\n",
    "            if 'imageUrl' in authorKeys:\n",
    "                imageUrl=author['imageUrl']\n",
    "            else:\n",
    "                imageUrl=''\n",
    "            if 'coverUrl' in authorKeys:\n",
    "                coverUrl=author['coverUrl']\n",
    "            else:\n",
    "                coverUrl=''\n",
    "            authors.append(Author(guid,name,email,source,location,imageUrl,coverUrl))\n",
    "            \n",
    "        #visual part\n",
    "        visualKeys=list(data['visual'].keys())\n",
    "      \n",
    "        guid= data['id']\n",
    "        \n",
    "        if 'type'in visualKeys:\n",
    "            type_= data['visual']['type']\n",
    "        else:\n",
    "            type_=''\n",
    "        \n",
    "        if 'contentType' in visualKeys:\n",
    "            contentType=data['visual']['contentType']\n",
    "        else:\n",
    "            contentType=''\n",
    "        \n",
    "        if 'generatingRenditions' in visualKeys:\n",
    "            generatingRenditions=data['visual']['generatingRenditions']\n",
    "        else:\n",
    "            generatingRenditions=False\n",
    "            \n",
    "        if 'url' in visualKeys:\n",
    "            url=data['visual']['url']\n",
    "        else:\n",
    "            url=''\n",
    "        if 'caption' in visualKeys:\n",
    "            try:\n",
    "                caption=data['visual']['caption']['fr']\n",
    "            except:\n",
    "                caption=''\n",
    "        else:\n",
    "            caption=''\n",
    "        if 'credit' in visualKeys:\n",
    "            #credit and language are nested one level deeper but it is always a 1 key 1 value dict\n",
    "            try:\n",
    "                credit =list(data['visual']['credits'].values())[0]\n",
    "                language=list(data['visual']['credits'].keys())[0]\n",
    "            except:\n",
    "                credit=''\n",
    "                language=''\n",
    "        else:\n",
    "            credit=''\n",
    "            language='' \n",
    "                \n",
    "        visuals.append(Visual(guid,type_, contentType,generatingRenditions,url,caption,credit,language))\n",
    "                              \n",
    "            \n",
    "        #article part \n",
    "        articleKeys=data.keys()\n",
    "        guid=data['id']\n",
    "        templateName=data['templateName']\n",
    "            \n",
    "        availableInPreview=data['availableInPreview']\n",
    "        url=data['url'] \n",
    "        creationDate=data['creationDate'][:-1] #must remove de 'Z' at the end of date fields\n",
    "        modificationDate=data['modificationDate'][:-1] \n",
    "        if 'title' in articleKeys:\n",
    "            try:\n",
    "                title=list(data['title'].values())[0]\n",
    "                \n",
    "            except:\n",
    "                title=''\n",
    "        else:\n",
    "            title=''\n",
    "    \n",
    "        try:\n",
    "            channel=list(data['channel'].values())[0] \n",
    "            \n",
    "        except:\n",
    "            channel=''\n",
    "            \n",
    "        try:\n",
    "            lead=list(data['lead'].values())[0]\n",
    "        except:\n",
    "            lead=''\n",
    "        \n",
    "        if 'chapters' in articleKeys:\n",
    "            chapters=''\n",
    "            for paragraph in data['chapters']:\n",
    "                paragraphKeys= paragraph.keys()\n",
    "                if 'text' in paragraphKeys:\n",
    "                    chapters=chapters+'\\n'+paragraph['text']['fr']\n",
    "                \n",
    "             \n",
    "        else:\n",
    "            chapters=[{'abc':'cba'}]\n",
    "        articles.append(Article(guid,templateName,availableInPreview,url,creationDate,modificationDate,title,channel,lead,chapters))                  \n",
    "      \n",
    "            \n",
    "     \n",
    "        jar.close() \n",
    "        return articles,authors,visuals  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataBase connections configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'user': 'root',\n",
    "    'password': 'abc123',\n",
    "    'host': '34.69.198.118',\n",
    "    'use_pure':True,\n",
    "    'database':'GLO_7027',\n",
    "    'client_flags': [ClientFlag.SSL],\n",
    "    'ssl_ca': 'server-ca.pem',\n",
    "    'ssl_cert': 'client-cert.pem',\n",
    "    'ssl_key': 'client-key.pem'\n",
    "}\n",
    "\n",
    "cnxn = mysql.connector.connect(**config)\n",
    "cursor = cnxn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database table configuration "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MySQL Queries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# create a new  database\n",
    "cursor = cnxn.cursor()  # initialize connection cursor\n",
    "cursor.execute('CREATE DATABASE GLO_7027')  # create a new  database\n",
    "cnxn.close()  # close connection because we will be reconnecting to testdb\n",
    "\n",
    "\n",
    "config['database'] = 'GLO_7027'  # add new database to config dict\n",
    "cnxn = mysql.connector.connect(**config)\n",
    "cursor = cnxn.cursor()\n",
    "\n",
    "#drop a table\n",
    "cursor.execute(\"DROP TABLE XXX\")\n",
    "cnxn.commit()\n",
    "\n",
    "#create analytics table\n",
    "cursor.execute(\"CREATE TABLE analytics (\"\n",
    "               \"id INT AUTO_INCREMENT PRIMARY KEY,\"\n",
    "               \"created_at DATETIME,\"\n",
    "               \"guid VARCHAR(50),\"\n",
    "               \"hash VARCHAR(255),\"\n",
    "               \"product VARCHAR(255),\"\n",
    "               \"interface VARCHAR(255) )\")\n",
    "\n",
    "#insert into analytics db\n",
    "insert_into_analytics = (\"INSERT INTO analytics (hash, click_time, product, created_at,interface) \"\n",
    "         \"VALUES (%s, %s, %s, %s,%s)\")\n",
    "\n",
    "\n",
    "#create article table\n",
    "cursor.execute(\"CREATE TABLE articles (\"\n",
    "               \"id INT AUTO_INCREMENT PRIMARY KEY,\"\n",
    "               \"guid VARCHAR(50),\"\n",
    "               \"templateName TEXT,\"\n",
    "               \"availableInPreview BOOL,\"\n",
    "               \"url VARCHAR(255),\"\n",
    "               \"creationDate DATETIME,\"\n",
    "               \"modificationDate DATETIME,\"\n",
    "               \"title TEXT,\"\n",
    "               \"channel TEXT,\"\n",
    "               \"lead TEXT,\"\n",
    "               \"articleText TEXT )\")\n",
    "\n",
    "#insert into articles db\n",
    "insert_into_articles = (\"INSERT INTO articles (guid, templateName, availableInPreview, url,creationDate,modificationDate,title,channel,lead,articleText) \"\n",
    "         \"VALUES (%s, %s, %s, %s,%s,%s, %s, %s, %s,%s)\")\n",
    "\n",
    "\n",
    "#create authors table\n",
    "\n",
    "cursor.execute(\"CREATE TABLE authors (\"\n",
    "               \"id INT AUTO_INCREMENT PRIMARY KEY,\"\n",
    "               \"guid VARCHAR(50),\"\n",
    "               \"name VARCHAR(255),\"\n",
    "               \"email VARCHAR(255),\"\n",
    "               \"source VARCHAR(255),\"\n",
    "               \"location VARCHAR(255),\"\n",
    "               \"imageUrl VARCHAR(255),\"\n",
    "               \"coverUrl VARCHAR(255) )\")\n",
    "\n",
    "insert_into_authors = (\"INSERT INTO authors (guid, name, email, source,location,imageUrl,coverUrl) \"\n",
    "         \"VALUES (%s, %s, %s, %s,%s,%s, %s)\")\n",
    "\n",
    "\n",
    "\n",
    "#create visual table\n",
    "Visual=namedtuple('Visual', 'guid type contentType generatingRenditions url caption credit language')\n",
    "\n",
    "cursor.execute(\"CREATE TABLE visuals (\"\n",
    "               \"id INT AUTO_INCREMENT PRIMARY KEY,\"\n",
    "               \"guid VARCHAR(50),\"\n",
    "               \"type VARCHAR(255),\"\n",
    "               \"contentType VARCHAR(255),\"\n",
    "               \"generatingRenditions BOOLEAN,\"\n",
    "               \"url VARCHAR(255),\"\n",
    "               \"caption TEXT,\"\n",
    "               \"credit VARCHAR(255),\"\n",
    "               \"language VARCHAR(255) )\")\n",
    "               \n",
    "insert_into_visuals = (\"INSERT INTO visuals (guid, type, contentType, generatingRenditions,url,caption,credit,language) \"\n",
    "         \"VALUES (%s, %s, %s, %s,%s,%s, %s,%s)\")\n",
    "\n",
    "cursor.execute(\"SELECT COUNT(*) from analytics\" )\n",
    "out=cursor.fetchall()\n",
    "out\n",
    "\n",
    "cnxn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inputs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section contains key variables used as inputs to create the final data uploaded to SQL databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#process JSON files\n",
    "\n",
    "#Inputs for analytics file\n",
    "start_time = time.process_time()\n",
    "#if it is the first time\n",
    "processed_files=[]\n",
    "#if not then upload the list of already processed files\n",
    "#processed_files=pd.read_csv('processed_files_lenouvelliste.csv')['file_name'].tolist()\n",
    "files=os.listdir('/Users/gabounet/ULaval/GLO_2027/TrainSample/')\n",
    "total=len(files)\n",
    "#path_to_file='/Users/gabounet/ULaval/GLO_2027/RichardKhoury/Analytics/lavoixdelest/'\n",
    "\n",
    "# inputs for query\n",
    "step=50000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treatment and upload analytics file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#process JSON files\n",
    "\n",
    "\n",
    "\n",
    "count=1\n",
    "for file in files[:]:\n",
    "    \n",
    "    if file not in processed_files:\n",
    "        #try to open the json if it doesn't have a bug\n",
    "        try:\n",
    "            output=process_analytic_file(path_to_file,file)\n",
    "\n",
    "        except:\n",
    "            output=[]\n",
    "        #load data to google mysqldb by batch equal to the step to avoid connection drop\n",
    "        \n",
    "        start=0\n",
    "        limit=len(output)\n",
    "        while start< limit:\n",
    "\n",
    "            cursor.executemany(insert_into_analytics, output[start:min(limit,start+step)])\n",
    "            cnxn.commit()  # and commit changes\n",
    "            start=start+step\n",
    "\n",
    "\n",
    "        processed_files.append(file)\n",
    "        print(file,\" \",count,\" ouf of \",total)\n",
    "        count=count+1\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treatment and upload of  Article,Authors and Viz tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#process JSON files\n",
    "\n",
    "#Inputs for analytics file\n",
    "start_time = time.process_time()\n",
    "#if it is the first time\n",
    "processed_files=[]\n",
    "#if not then upload the list of already processed files\n",
    "#processed_files=pd.read_csv('processed_files_lenouvelliste.csv')['file_name'].tolist()\n",
    "files=os.listdir('/Users/gabounet/ULaval/GLO_2027/RichardKhoury/train/')\n",
    "total=len(files)\n",
    "#path_to_file='/Users/gabounet/ULaval/GLO_2027/RichardKhoury/Analytics/lavoixdelest/'\n",
    "correctFiles=[f for f in files if '--pub' not in f] # only files not containing \"--pub-infos\"\n",
    "# inputs for query\n",
    "step=50000\n",
    "limit=len(correctFiles)\n",
    "output_path='/Users/gabounet/ULaval/GLO_2027/GLO_2027_work/outputs_to_db/'\n",
    "input_path='/Users/gabounet/ULaval/GLO_2027/RichardKhoury/train/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "100000\n",
      "37.431827999999996\n",
      "150000\n",
      "200000\n",
      "92.58610199999998\n",
      "250000\n",
      "300000\n",
      "146.013508\n",
      "350000\n",
      "400000\n",
      "202.91644799999995\n",
      "450000\n",
      "500000\n",
      "259.8742199999999\n",
      "550000\n",
      "310.52235199999996\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start_time = time.process_time()\n",
    "n=1\n",
    "start=0\n",
    "step=100000\n",
    "limit=len(correctFiles)\n",
    "\n",
    "while start< limit:\n",
    "    art_list=[]\n",
    "    auth_list=[]\n",
    "    viz_list=[]\n",
    "    \n",
    "    # select file in the range and get date to then store in lists\n",
    "    for file in correctFiles[start:min(start+step,limit)]:\n",
    "        art,auth,viz=process_article_file(input_path,file)\n",
    "        #print(n)\n",
    "        for article in art:\n",
    "            art_list.append(article) \n",
    "        #for author in auth:\n",
    "            #auth_list.append(author)\n",
    "        #for vizu in viz:\n",
    "            #viz_list.append(vizu)\n",
    "        n=n+1\n",
    "        if n%50000==0:\n",
    "            print(n)\n",
    "    #increment the starting point\n",
    "    \n",
    "\n",
    "    #dump to csv\n",
    "    #articles\n",
    "    fileName='articles_data_V2'+'_'+str(start)+'-'+str(min(start+step,limit))+'.csv'\n",
    "    pd.DataFrame.from_records(art_list,columns=art_list[0]._fields).to_csv(output_path+fileName,index=False)\n",
    "    start=start+step\n",
    "    \n",
    "    #authors\n",
    "    #fileName='authors_data'+'_'+str(start)+'-'+str(stop)+'.csv'\n",
    "    #pd.DataFrame.from_records(auth_list,columns=auth_list[0]._fields).to_csv(output_path+fileName,index=False)\n",
    "    #vizuals\n",
    "    #fileName='visuals_data'+'_'+str(start)+'-'+str(stop)+'.csv'\n",
    "\n",
    "    #pd.DataFrame.from_records(viz_list,columns=viz_list[0]._fields).to_csv(output_path+fileName,index=False)\n",
    "    \n",
    "    print(time.process_time()-start_time)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## process articles files by chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['articles_data_V2_300000-400000.csv',\n",
       " 'articles_data_V2_200000-300000.csv',\n",
       " 'articles_data_V2_400000-500000.csv',\n",
       " 'articles_data_V2_500000-592820.csv',\n",
       " 'articles_data_V2_100000-200000.csv',\n",
       " 'articles_data_V2_600000-592820.csv']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files=os.listdir('/Users/gabounet/ULaval/GLO_2027/GLO_2027_work/outputs_to_db/')\n",
    "filesToProcess=[f for f in files if 'V2' in f]\n",
    "filesToProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "files=os.listdir('/Users/gabounet/ULaval/GLO_2027/GLO_2027_work/outputs_to_db/')\n",
    "\n",
    "\n",
    "for file in filesToProcess:\n",
    "    path='/Users/gabounet/ULaval/GLO_2027/GLO_2027_work/outputs_to_db/'\n",
    "    fileName=file\n",
    "    dataToProcess=path+fileName\n",
    "    insert_into_articles = (\"INSERT INTO articles (guid, templateName, availableInPreview, url,creationDate,modificationDate,title,channel,lead) \"\n",
    "             \"VALUES (%s, %s, %s, %s,%s,%s, %s, %s, %s)\")\n",
    "\n",
    "\n",
    "\n",
    "    n=0\n",
    "    for chunk in pd.read_csv(dataToProcess, chunksize=5000):\n",
    "\n",
    "\n",
    "        #data cleaning\n",
    "        chunk['availableInPreview'] = chunk['availableInPreview'].fillna(0)\n",
    "        chunk=chunk.replace(np.nan, '', regex=True)\n",
    "        del chunk['chapters']\n",
    "        output_to_db=list(chunk.itertuples(index=False, name=None))\n",
    "        output_to_db=[output for output in output_to_db if output[0]!='']\n",
    "\n",
    "        cursor.executemany(insert_into_articles, output_to_db)\n",
    "        cnxn.commit()  \n",
    "        n=n+1\n",
    "        print(n)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
